{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Engineer Technical Assessment\n",
    "\n",
    "## Overview\n",
    "Build an AI-powered solution for sentiment analysis of movie reviews that leverages the existing dataset to improve accuracy. This assessment is designed to be completed in 2-3 hours, we do NOT expect very detailed answers or long explanations.\n",
    "\n",
    "## Notes\n",
    "- AI assistance is allowed and, in fact, encouraged. caveats are:\n",
    "    - Concise explanations and simple code are preferred\n",
    "    - Solutions that use newer information and go beyond LLMs cuttof date are valuable.\n",
    "    - You must be able to explain the code you write here\n",
    "\n",
    "- Look up any information you need, copy and paste code is allowed.\n",
    "- Setup the environment as needed. You can use your local environment, colab, or any other environment of your preferenc.\n",
    "- Focus on working solutions, leave iteration and improvements if you have extra time.\n",
    "\n",
    "## Setup\n",
    "The following cells will download and prepare the IMDB dataset. "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T10:53:17.996411Z",
     "start_time": "2025-05-18T10:52:59.123323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Sample subset for quicker development\n",
    "train_df = train_df.sample(n=5000, random_state=42)\n",
    "test_df = test_df.sample(n=10, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample review:\")\n",
    "sample = train_df.iloc[0]\n",
    "print(f\"Text: {sample['text'][:200]}...\")\n",
    "print(f\"Sentiment: {'Positive' if sample['label'] == 1 else 'Negative'}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Trabajos\\Kelea\\prueba_tecnica_inditex\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Test samples: 10\n",
      "\n",
      "Sample review:\n",
      "Text: Dumb is as dumb does, in this thoroughly uninteresting, supposed black comedy. Essentially what starts out as Chris Klein trying to maintain a low profile, eventually morphs into an uninspired version...\n",
      "Sentiment: Negative\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1: Model Implementation\n",
    "Implement a solution that analyzes sentiment in movie reviews. This part is explicitly open-ended: Explore ways to leverage the example dataset to enhance predictions. You can consider a pre-trained language model that can understand and generate text, external API's, RAG systems etc. \n",
    "Feel free to use any library or tool you are comfortable with."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T11:07:32.268604Z",
     "start_time": "2025-05-18T11:07:32.228605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, PreTrainedTokenizerFast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T11:07:33.564536Z",
     "start_time": "2025-05-18T11:07:33.540536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Hugging Face token from .env file\n",
    "load_dotenv()\n",
    "print(\"Loaded .env file.\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T10:53:46.277556Z",
     "start_time": "2025-05-18T10:53:46.249554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T10:55:06.450747Z",
     "start_time": "2025-05-18T10:55:06.430770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define model name - as we are going to fine-tune a LLM model with LoRA, Llama 3 8B as it's a good balance\n",
    "# of performance and resource requirements\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T11:10:35.343405100Z",
     "start_time": "2025-05-18T11:09:17.963538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if we're using CUDA to determine the quantization approach\n",
    "if device.type == \"cuda\":\n",
    "    # Set up quantization configuration for efficient memory usage\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                     # Load the model weights in 4-bit format\n",
    "        bnb_4bit_quant_type=\"nf4\",             # Use the \"nf4\" quantization type\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # Use the float16 data type for computations\n",
    "        bnb_4bit_use_double_quant=True,        # Use double quantization for better accuracy\n",
    "    )\n",
    "\n",
    "    # Load the model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    # For CPU, load a smaller model or with different settings\n",
    "    print(\"Running on CPU. This will be slow and may require a smaller model.\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU. This will be slow and may require a smaller model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Trabajos\\Kelea\\prueba_tecnica_inditex\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare the model for training:\n",
    "#   1. Freezes all parameters\n",
    "#   2. Cast output embeddings and LayerNorm weights to float32\n",
    "#   3. Enables gradient checkpointing\n",
    "#   4. Add the upcasting of the lm head to float32\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set up LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                                                    # Rank - Lora attention dimension\n",
    "    lora_alpha=32,                                           # Scaling factor or learning rate for Lora weights\n",
    "    lora_dropout=0.05,                                       # Dropout probability for Lora layers\n",
    "    bias=\"none\",                                             # Type of bias to use\n",
    "    task_type=\"CAUSAL_LM\",                                   # Type of task that the model is being trained for\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",]    # Target attention modules\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"Model prepared with LoRA\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_data(df: pd.DataFrame, tokenizer: PreTrainedTokenizerFast, max_length: int = 512) -> List[Dict]:\n",
    "    \"\"\"Preprocess data for training by formatting prompts and tokenizing.\n",
    "\n",
    "    Args:\n",
    "        df (Dataframe): Dataframe containing the data.\n",
    "        tokenizer (PreTrainedTokenizerFast): Tokenizer to use for preprocessing.\n",
    "        max_length (int, optional): Maximum length of the input sequence. Defaults to 512.\n",
    "\n",
    "    Returns:\n",
    "        (List[Dict]): List of preprocessed data items.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preprocessing data\"):\n",
    "        text = row['text']\n",
    "        label = \"positive\" if row['label'] == 1 else \"negative\"\n",
    "\n",
    "        # Create the prompt in the format Llama 3 expects\n",
    "        prompt = f\"<|begin_of_text|>Review: {text}\\nSentiment: {label}<|end_of_text|>\"\n",
    "\n",
    "        # Tokenize\n",
    "        encodings = tokenizer(prompt, truncation=True, max_length=max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        processed_data.append({\n",
    "            \"input_ids\": encodings[\"input_ids\"][0],\n",
    "            \"attention_mask\": encodings[\"attention_mask\"][0],\n",
    "            \"labels\": encodings[\"input_ids\"][0].clone(),\n",
    "            \"original_text\": text,\n",
    "            \"original_label\": row['label']\n",
    "        })\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocess training data\n",
    "print(\"Preprocessing training data...\")\n",
    "train_data = preprocess_data(train_df, tokenizer)\n",
    "print(f\"Processed {len(train_data)} training examples\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a simple dataset class\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.data[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.data[idx][\"attention_mask\"],\n",
    "            \"labels\": self.data[idx][\"labels\"]\n",
    "        }"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create the training dataset and dataloader\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,     # Small batch size due to memory constraints\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model: any, dataloader: torch.utils.data.DataLoader, num_epochs: int = 3) -> any:\n",
    "    \"\"\"Train the model using the provided dataloader. Displays a chart of training loss over time.\n",
    "\n",
    "    Args:\n",
    "        model (any): The model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to be used for training.\n",
    "        num_epochs (int, optional): The number of epochs to train for. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        (any): The trained model.\n",
    "    \"\"\"\n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs+1), losses, marker='o')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "trained_model = train_model(model, train_dataloader, num_epochs=5)\n",
    "print(\"Model training completed\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_sentiment(text: str, model: any, tokenizer: PreTrainedTokenizerFast, max_length: int = 512) -> Dict[str, str | float]:\n",
    "    \"\"\"Predict sentiment for a given text. Also returns the confidence score.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be analyzed.\n",
    "        model (any): The model to be used for prediction.\n",
    "        tokenizer (PreTrainedTokenizerFast): The tokenizer to be used for preprocessing.\n",
    "        max_length (int, optional): The maximum length of the input sequence. Defaults to 512.\n",
    "    \n",
    "    Returns:\n",
    "        (Dict[str, str | float]): A dictionary containing the predicted sentiment and confidence score. \n",
    "    \"\"\"\n",
    "    # Create prompt\n",
    "    prompt = f\"<|begin_of_text|>Review: {text}\\nSentiment:\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            temperature=0.1,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract sentiment\n",
    "    try:\n",
    "        sentiment_part = generated_text.split(\"Sentiment:\")[-1].strip().lower()\n",
    "        if \"positive\" in sentiment_part:\n",
    "            sentiment = \"positive\"\n",
    "            confidence = 0.9  # Simplified confidence score\n",
    "        elif \"negative\" in sentiment_part:\n",
    "            sentiment = \"negative\"\n",
    "            confidence = 0.9  # Simplified confidence score\n",
    "        else:\n",
    "            # If the model output is unclear, use a heuristic approach\n",
    "            positive_words = [\"good\", \"great\", \"excellent\", \"amazing\", \"enjoyed\", \"best\", \"recommend\"]\n",
    "            negative_words = [\"bad\", \"terrible\", \"awful\", \"worst\", \"disappointing\", \"waste\", \"boring\"]\n",
    "\n",
    "            pos_count = sum(1 for word in positive_words if word in text.lower())\n",
    "            neg_count = sum(1 for word in negative_words if word in text.lower())\n",
    "\n",
    "            if pos_count > neg_count:\n",
    "                sentiment = \"positive\"\n",
    "                confidence = 0.6 + (0.1 * min(pos_count - neg_count, 3))  # Scale confidence\n",
    "            else:\n",
    "                sentiment = \"negative\"\n",
    "                confidence = 0.6 + (0.1 * min(neg_count - pos_count, 3))  # Scale confidence\n",
    "    except:\n",
    "        # Fallback\n",
    "        sentiment = \"unknown\"\n",
    "        confidence = 0.5\n",
    "\n",
    "    return {\n",
    "        \"sentiment\": sentiment,\n",
    "        \"confidence\": confidence\n",
    "    }"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test the model on a sample\n",
    "sample_review = \"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\"\n",
    "result = predict_sentiment(sample_review, trained_model, tokenizer)\n",
    "print(f\"Sample review: {sample_review}\")\n",
    "print(f\"Predicted sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save model fine-tuned\n",
    "print(\"Saving fine-tuned model...\")\n",
    "trained_model.save_pretrained(\"fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: API Implementation\n",
    "Create a simple API using FastAPI that serves your solution. The API should accept a review text and return the sentiment analysis result.\n",
    "\n",
    "Expected format:\n",
    "```python\n",
    "# Request\n",
    "{\n",
    "    \"review_text\": \"This movie exceeded my expectations...\"\n",
    "}\n",
    "\n",
    "# Response\n",
    "{\n",
    "    \"sentiment\": \"positive\",\n",
    "    \"confidence\": 0.92,\n",
    "    \"similar_reviews\": [\n",
    "        {},\n",
    "        {}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import datetime"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define request and response models\n",
    "class ReviewRequest(BaseModel):\n",
    "    review_text: str\n",
    "\n",
    "class SimilarReview(BaseModel):\n",
    "    text: str\n",
    "    sentiment: str\n",
    "    similarity: float\n",
    "\n",
    "class SentimentResponse(BaseModel):\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "    similar_reviews: List[SimilarReview]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create FastAPI app\n",
    "api_version = \"1.0.0\"\n",
    "app = FastAPI(title=\"Movie Review Sentiment Analysis API\",\n",
    "              description=\"API for analyzing sentiment in movie reviews using Llama 3 with LoRA fine-tuning\",\n",
    "              version=api_version)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Global variables to store model and data\n",
    "global_model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_model\")\n",
    "global_tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "global_train_data = train_data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Root endpoint\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Welcome to the Movie Review Sentiment Analysis API\"}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Health check endpoint\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"version\": api_version,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"model_loaded\": global_model is not None\n",
    "    }"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_similar_reviews(text: str, train_data: List[Dict], top_n: int = 2) -> List[Dict]:\n",
    "    \"\"\"Find similar reviews in the training data using simple word overlap.\n",
    "\n",
    "    Args:\n",
    "        text (str): The review text.\n",
    "        train_data (List[Dict]): The training data.\n",
    "        top_n (int, optional): The number of top results. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        (List[Dict]): A list of dictionaries containing the result.\n",
    "    \"\"\"\n",
    "    text_words = set(text.lower().split())\n",
    "    similarities = []\n",
    "\n",
    "    for item in train_data:\n",
    "        review_words = set(item[\"original_text\"].lower().split())\n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(text_words.intersection(review_words))\n",
    "        union = len(text_words.union(review_words))\n",
    "        similarity = intersection / union if union > 0 else 0\n",
    "\n",
    "        similarities.append({\n",
    "            \"text\": item[\"original_text\"],\n",
    "            \"sentiment\": \"positive\" if item[\"original_label\"] == 1 else \"negative\",\n",
    "            \"similarity\": similarity\n",
    "        })\n",
    "\n",
    "    # Sort by similarity and return top_n\n",
    "    similar_reviews = sorted(similarities, key=lambda x: x[\"similarity\"], reverse=True)[:top_n]\n",
    "    return similar_reviews"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def complete_prediction(text: str, model: any, tokenizer: PreTrainedTokenizerFast, train_data: List[Dict]) -> Dict[str, any]:\n",
    "    \"\"\"Complete prediction function that returns sentiment, confidence, and similar reviews.\n",
    "\n",
    "    Args:\n",
    "        text (str): The review text.\n",
    "        model (any): The model to be used.\n",
    "        tokenizer (PreTrainedTokenizerFast): The tokenizer to be used for preprocessing.\n",
    "        train_data (List[Dict]): The training data.\n",
    "\n",
    "    Returns:\n",
    "        (Dict[str, any]): A dictionary containing the result.\n",
    "    \"\"\"\n",
    "    # Get sentiment prediction\n",
    "    result = predict_sentiment(text, model, tokenizer)\n",
    "\n",
    "    # Find similar reviews\n",
    "    similar_reviews = find_similar_reviews(text, train_data)\n",
    "\n",
    "    return {\n",
    "        \"sentiment\": result[\"sentiment\"],\n",
    "        \"confidence\": result[\"confidence\"],\n",
    "        \"similar_reviews\": similar_reviews\n",
    "    }"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sentiment analysis endpoint\n",
    "@app.post(\"/analyze\", response_model=SentimentResponse)\n",
    "def analyze_sentiment(request: ReviewRequest):\n",
    "    try:\n",
    "        # Check if the text is provided\n",
    "        if not request.review_text or len(request.review_text.strip()) == 0:\n",
    "            raise HTTPException(status_code=400, detail=\"Review text cannot be empty\")\n",
    "\n",
    "        # Get prediction\n",
    "        result = complete_prediction(\n",
    "            text=request.review_text,\n",
    "            model=global_model,\n",
    "            tokenizer=global_tokenizer,\n",
    "            train_data=global_train_data\n",
    "        )\n",
    "\n",
    "        # Format similar reviews for response\n",
    "        similar_reviews = [\n",
    "            SimilarReview(\n",
    "                text=review[\"text\"],\n",
    "                sentiment=review[\"sentiment\"],\n",
    "                similarity=review[\"similarity\"]\n",
    "            )\n",
    "            for review in result[\"similar_reviews\"]\n",
    "        ]\n",
    "\n",
    "        # Return response\n",
    "        return SentimentResponse(\n",
    "            sentiment=result[\"sentiment\"],\n",
    "            confidence=result[\"confidence\"],\n",
    "            similar_reviews=similar_reviews\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Change to a log error in a production environment\n",
    "        print(f\"Error processing request: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing request: {str(e)}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to run the API server\n",
    "def main():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example of how to start the API server\n",
    "print(\"To start the API server, run:\")\n",
    "print(\"main()\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example of API usage with curl\n",
    "print(\"\\nExample curl request:\")\n",
    "print('''curl -X POST \"http://localhost:8000/analyze\" \\\\\n",
    "    -H \"Content-Type: application/json\" \\\\\n",
    "    -d '{\"review_text\": \"This movie was absolutely fantastic! The acting was superb.\"}'\n",
    "''')\n",
    "\n",
    "# Example of API usage with Python\n",
    "print(\"\\nExample of API usage in Python:\")\n",
    "print('''\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8000/analyze\"\n",
    "data = {\"review_text\": \"This movie was absolutely fantastic! The acting was superb.\"}\n",
    "response = requests.post(url, json=data)\n",
    "result = response.json()\n",
    "print(json.dumps(result, indent=2))\n",
    "''')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 3: Testing and Performance\n",
    "Evaluate your solution's performance on the test set. Include:\n",
    "1. Accuracy metrics (precision, recall, F1-score)\n",
    "2. Inference speed (average time per prediction)\n",
    "\n",
    "Compare performance with and without using the example data to demonstrate any improvements."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "from transformers import pipeline"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocess test data\n",
    "print(\"Preprocessing test data...\")\n",
    "test_data = preprocess_data(test_df, tokenizer)\n",
    "print(f\"Processed {len(test_data)} test examples\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(model: any, test_data: List[Dict], tokenizer: Optional[PreTrainedTokenizerFast] = None) -> Dict:\n",
    "    \"\"\"Evaluate the model performance on test data.\n",
    "\n",
    "    Args:\n",
    "        model (any): The model to be used.\n",
    "        test_data (List[Dict]): The test data.\n",
    "        tokenizer (Optional[PreTrainedTokenizerFast]): The tokenizer to be used for preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        (Dict): A dictionary containing all the metrics.\n",
    "    \"\"\"\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    confidences = []\n",
    "    inference_times = []\n",
    "\n",
    "    print(\"Evaluating model on test data...\")\n",
    "    for item in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        # Get the true label\n",
    "        true_label = \"positive\" if item[\"original_label\"] == 1 else \"negative\"\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        if tokenizer is None:\n",
    "            result = model(item[\"original_text\"])[0]\n",
    "        else:\n",
    "            result = predict_sentiment(item[\"original_text\"], model, tokenizer)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Record results\n",
    "        predicted_labels.append(result[\"sentiment\"])\n",
    "        confidences.append(result[\"confidence\"])\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score([1 if label == \"positive\" else 0 for label in true_labels],\n",
    "                             [1 if label == \"positive\" else 0 for label in predicted_labels])\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        [1 if label == \"positive\" else 0 for label in true_labels],\n",
    "        [1 if label == \"positive\" else 0 for label in predicted_labels],\n",
    "        average='binary'\n",
    "    )\n",
    "\n",
    "    avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Average Inference Time: {avg_inference_time:.4f} seconds per review\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        [1 if label == \"positive\" else 0 for label in true_labels],\n",
    "        [1 if label == \"positive\" else 0 for label in predicted_labels],\n",
    "        target_names=[\"Negative\", \"Positive\"]\n",
    "    ))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"avg_inference_time\": avg_inference_time,\n",
    "        \"true_labels\": true_labels,\n",
    "        \"predicted_labels\": predicted_labels,\n",
    "        \"confidences\": confidences,\n",
    "        \"inference_times\": inference_times\n",
    "    }"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate fine-tuned model\n",
    "print(\"\\n=== Evaluating Fine-tuned Llama 3 Model with LoRA ===\")\n",
    "fine_tuned_results = evaluate_model(trained_model, test_data, tokenizer)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For comparison, let's use a baseline model (BERT-based sentiment classifier)\n",
    "print(\"\\n=== Evaluating Baseline Model (BERT) ===\")\n",
    "# Load a pre-trained sentiment analysis model\n",
    "baseline_model = pipeline(\"sentiment-analysis\", device=0 if torch.cuda.is_available() else -1)\n",
    "baseline_results = evaluate_model(baseline_model, test_data)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compare models\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(f\"Metric          | Fine-tuned Llama 3 | Baseline BERT\")\n",
    "print(f\"----------------|-------------------|-------------\")\n",
    "print(f\"Accuracy        | {fine_tuned_results['accuracy']:.4f}              | {baseline_results['accuracy']:.4f}\")\n",
    "print(f\"Precision       | {fine_tuned_results['precision']:.4f}              | {baseline_results['precision']:.4f}\")\n",
    "print(f\"Recall          | {fine_tuned_results['recall']:.4f}              | {baseline_results['recall']:.4f}\")\n",
    "print(f\"F1 Score        | {fine_tuned_results['f1']:.4f}              | {baseline_results['f1']:.4f}\")\n",
    "print(f\"Inference Time  | {fine_tuned_results['avg_inference_time']:.4f} sec          | {baseline_results['avg_inference_time']:.4f} sec\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize results\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "fine_tuned_scores = [fine_tuned_results['accuracy'], fine_tuned_results['precision'],\n",
    "                     fine_tuned_results['recall'], fine_tuned_results['f1']]\n",
    "baseline_scores = [baseline_results['accuracy'], baseline_results['precision'],\n",
    "                   baseline_results['recall'], baseline_results['f1']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, fine_tuned_scores, width, label='Fine-tuned Llama 3')\n",
    "plt.bar(x + width/2, baseline_scores, width, label='Baseline BERT')\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize inference time comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "models = ['Fine-tuned Llama 3', 'Baseline BERT']\n",
    "inference_times = [fine_tuned_results['avg_inference_time'], baseline_results['avg_inference_time']]\n",
    "\n",
    "plt.bar(models, inference_times, color=['#1f77b4', '#ff7f0e'])\n",
    "plt.ylabel('Average Inference Time (seconds)')\n",
    "plt.title('Inference Speed Comparison')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze confidence distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(fine_tuned_results['confidences'], bins=10, alpha=0.7, label='Fine-tuned Llama 3')\n",
    "plt.hist(baseline_results['confidences'], bins=10, alpha=0.7, label='Baseline BERT')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Confidence Score Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Conclusion\n",
    "print(\"\\n=== Performance Analysis Conclusion ===\")\n",
    "if fine_tuned_results['f1'] > baseline_results['f1']:\n",
    "    print(\"The fine-tuned Llama 3 model with LoRA outperforms the baseline BERT model in terms of F1 score.\")\n",
    "    print(f\"Improvement: {(fine_tuned_results['f1'] - baseline_f1) / baseline_f1 * 100:.2f}% increase in F1 score.\")\n",
    "else:\n",
    "    print(\"The baseline BERT model outperforms the fine-tuned Llama 3 model in terms of F1 score.\")\n",
    "    print(f\"Difference: {(baseline_f1 - fine_tuned_results['f1']) / fine_tuned_results['f1'] * 100:.2f}% higher F1 score for baseline.\")\n",
    "\n",
    "if fine_tuned_results['avg_inference_time'] < baseline_avg_inference_time:\n",
    "    print(\"\\nThe fine-tuned Llama 3 model is faster for inference.\")\n",
    "    print(f\"Speed improvement: {(baseline_avg_inference_time - fine_tuned_results['avg_inference_time']) / baseline_avg_inference_time * 100:.2f}% faster.\")\n",
    "else:\n",
    "    print(\"\\nThe baseline BERT model is faster for inference.\")\n",
    "    print(f\"Speed difference: {(fine_tuned_results['avg_inference_time'] - baseline_avg_inference_time) / baseline_avg_inference_time * 100:.2f}% slower for fine-tuned model.\")\n",
    "\n",
    "print(\"\\nKey advantages of the fine-tuned Llama 3 model:\")\n",
    "print(\"1. Better understanding of context and nuanced language in movie reviews\")\n",
    "print(\"2. Ability to provide similar reviews for context\")\n",
    "print(\"3. More flexible and adaptable to different types of review language\")\n",
    "print(\"4. Can be further improved with more training data and longer training time\")\n",
    "\n",
    "print(\"\\nTrade-offs:\")\n",
    "print(\"1. Resource requirements (memory, computation)\")\n",
    "print(\"2. Training time and complexity\")\n",
    "print(\"3. Deployment considerations for larger models\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 4: Deployment Strategy\n",
    "\n",
    "1. Describe your deployment strategy considering:\n",
    "   - Data storage and retrieval\n",
    "   - Scalability\n",
    "   - Resource requirements\n",
    "   - Cost considerations\n",
    "\n",
    "2. Create a simple Dockerfile to package your solution"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Write your deployment strategy here as a markdown cell\n",
    "deployment_strategy = \"\"\"\n",
    "# Deployment Strategy for Sentiment Analysis API\n",
    "\n",
    "## Infrastructure\n",
    "\n",
    "* Compute: Amazon ECS with EC2 launch type (for GPU support)\n",
    "* Container orchestration: Docker container with the FastAPI app.\n",
    "* Networking: ECS Service with Application Load Balancer (ALB).\n",
    "* CI/CD: GitHub Actions to deploy container image to Amazon ECR and update ECS task.\n",
    "\n",
    "## Scalability Approach\n",
    "\n",
    "1. Horizontal Scaling:\n",
    "   - Scale tasks up/down based on CPU/memory usage and request rate\n",
    "\n",
    "2. Performance Optimization:\n",
    "   - Model quantization (4-bit) to reduce memory footprint\n",
    "   - Batch processing for high-throughput scenarios\n",
    "   - Caching frequently requested predictions\n",
    "\n",
    "3. High Availability:\n",
    "   - Multi-zone deployment for fault tolerance\n",
    "   - Health checks to ensure service is healthy before traffic is routed to it\n",
    "   - Graceful degradation with fallback models if primary model is unavailable\n",
    "\n",
    "## Model & Data Storage\n",
    "\n",
    "1. Model Storage:\n",
    "   - Store model artifacts in cloud object storage (S3)\n",
    "   - Version control for models using DVC or similar tools\n",
    "   - Model registry to track model versions and performance metrics\n",
    "\n",
    "2. Data Management:\n",
    "   - MongoDB for storing processed reviews and metadata\n",
    "   - Redis for caching frequent predictions and similar reviews\n",
    "   - Periodic data archiving for historical analysis\n",
    "   - Data versioning to track dataset changes\n",
    "\n",
    "3. Secrets Management:\n",
    "   - Securely store API keys and other sensitive data\n",
    "   - Use AWS Secrets Manager to manage secrets\n",
    "\n",
    "## Resource & Cost Considerations\n",
    "\n",
    "1. Resource Optimization:\n",
    "   - Right-sizing containers based on workload patterns\n",
    "   - Spot instances for batch processing and training\n",
    "   - Reserved instances for baseline capacity\n",
    "   - Autoscaling to match demand patterns\n",
    "\n",
    "2. Cost Management:\n",
    "   - Monitoring and alerting on resource usage\n",
    "   - Cost allocation tagging for different components\n",
    "   - Regular review of resource utilization\n",
    "   - Scheduled scaling for predictable traffic patterns\n",
    "\n",
    "3. Performance vs. Cost Tradeoffs:\n",
    "   - Quantization to reduce compute requirements\n",
    "   - Caching strategy to reduce inference calls\n",
    "   - Tiered service levels based on response time requirements\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_strategy)\n",
    "\n",
    "# Write your Dockerfile content\n",
    "dockerfile_content = \"\"\"\n",
    "# Use NVIDIA CUDA base image for GPU support\n",
    "FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install Python and dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    python3.10 \\\n",
    "    python3-pip \\\n",
    "    python3-dev \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements file\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip3 install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy model files and application code\n",
    "COPY ./model /app/model\n",
    "COPY ./app /app/app\n",
    "\n",
    "# Set environment variables\n",
    "ENV MODEL_PATH=/app/model\n",
    "ENV PYTHONPATH=/app\n",
    "\n",
    "# Expose port for API\n",
    "EXPOSE 8000\n",
    "\n",
    "# Set up entrypoint script\n",
    "COPY entrypoint.sh /app/\n",
    "RUN chmod +x /app/entrypoint.sh\n",
    "\n",
    "# Run the API server\n",
    "ENTRYPOINT [\"/app/entrypoint.sh\"]\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nDockerfile:\")\n",
    "print(dockerfile_content)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation Criteria\n",
    "- Implementation that can process reviews and return sentiments\n",
    "- Use of extra data to improve predictions\n",
    "- Proper API design\n",
    "- Reasonable deployment strategy\n",
    "\n",
    "Good luck!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
